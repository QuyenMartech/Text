import time
from datetime import datetime
import random
import json
import csv
import re
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchWindowException
from collections import defaultdict, deque
from typing import Dict, List
from selenium.webdriver.common.action_chains import ActionChains
import os
import threading
import math


# Configuration parameters
MIN_DELAY_BETWEEN_REQUESTS = 0.5
MAX_DELAY_BETWEEN_REQUESTS = 1
MAX_RETRIES = 3
PAGE_LOAD_TIMEOUT = 10
TAB_SWITCH_DELAY = 0.2
MAX_TABS = 5
SCROLL_PAUSE_TIME = 2

def calculate_std_dev(values):
    n = len(values)
    if n < 2:
        return 0
    mean = sum(values) / n
    squared_diff_sum = sum((x - mean) ** 2 for x in values)
    return math.sqrt(squared_diff_sum / (n - 1))

class ChromeInstance:
    def __init__(self, debug_port, worker_id):
        self.debug_port = debug_port
        self.worker_id = worker_id
        self.driver = None
        self.processed_count = 0
        self.is_checkpoint = False
        self.is_blocked = False
        self.processed_groups = set()
        self.collected_urls = []
        self.failed_urls = []

    def setup_chrome_driver(self):
        chrome_options = Options()
        chrome_options.add_experimental_option("debuggerAddress", f"127.0.0.1:{self.debug_port}")
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-notifications')
        chrome_options.add_argument('--disable-infobars')
        chrome_options.add_argument('--lang=vi')
        return chrome_options

    def create_driver(self):
        try:
            options = self.setup_chrome_driver()
            service = Service()
            self.driver = webdriver.Chrome(service=service, options=options)
            self.driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)
            return self.driver
        except Exception as e:
            raise Exception(f"Lỗi khởi tạo Chrome WebDriver port {self.debug_port}: {str(e)}")
			
def close_block_popup(self):
    try:
        popup = self.driver.find_element(By.XPATH, "//div[contains(@class, 'x1ey2m1c') and contains(@class, 'xds687c') and contains(@class, 'x17qophe') and contains(@class, 'xg01cxk') and contains(@class, 'x47corl') and contains(@class, 'x10l6tqk') and contains(@class, 'x13vifvy') and contains(@class, 'x1ebt8du') and contains(@class, 'x19991ni') and contains(@class, 'x1dhq9h') and contains(@class, 'xzolkzo') and contains(@class, 'x12go9s9') and contains(@class, 'x1rnf11y') and contains(@class, 'xprq8jg')]")
        actions = ActionChains(self.driver)
        actions.double_click(popup).perform()
        time.sleep(1)
        return True
    except:
        return False

    def check_checkpoint(self):
        try:
            current_url = self.driver.current_url.lower()
            checkpoint_indicators = [
                "facebook.com/checkpoint",
                "facebook.com/recover",
                "facebook.com/login",
                "/login/?next",
                "checkpoint/?next"
            ]
            return any(indicator in current_url for indicator in checkpoint_indicators)
        except:
            return False

    def check_blocked(self):
        try:
            blocked_text = "Bạn tạm thời bị chặn"
            blocked_text_2 = "Có vẻ như bạn đang dùng nhầm tính năng này do sử dụng quá nhanh"
            page_source = self.driver.page_source
            return blocked_text in page_source or blocked_text_2 in page_source
        except:
            return False

    def scroll_and_collect_groups(self):
        print(f"[Worker {self.worker_id}] Bắt đầu scroll và thu thập URLs...")
        group_links = set()
        no_new_content_count = 0
        previous_content_length = 0
        last_height = self.driver.execute_script("return document.documentElement.scrollHeight")
        
        while True:
            # Scroll xuống
            current_height = self.driver.execute_script("return window.pageYOffset")
            target_height = min(current_height + 300, last_height)
            self.driver.execute_script(f"window.scrollTo(0, {target_height});")
            time.sleep(0.3)

            # Đợi để trang load thêm nội dung
            time.sleep(SCROLL_PAUSE_TIME)
            
            # Kiểm tra vị trí scroll hiện tại
            actual_height = self.driver.execute_script("return window.pageYOffset")
            if abs(actual_height - target_height) > 50:  # Nếu vị trí scroll bị nhảy
                self.driver.execute_script(f"window.scrollTo(0, {target_height});")
                time.sleep(0.3)

            # Thu thập URLs
            elements = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='/groups/']")
            current_content_length = len(elements)
            
            print(f"[Worker {self.worker_id}] Đã tìm thấy {current_content_length} nhóm...")
            
            for element in elements:
                try:
                    href = element.get_attribute('href')
                    if href and re.search(r'facebook\.com/groups/\d+/?$', href):
                        clean_url = re.sub(r'\?.*$', '', href)
                        if clean_url not in self.processed_groups:
                            group_links.add(clean_url)
                except:
                    continue

            # Kiểm tra xem có nội dung mới không
            if current_content_length == previous_content_length:
                no_new_content_count += 1
                print(f"[Worker {self.worker_id}] Không tìm thấy nội dung mới... ({no_new_content_count}/8)")
                if no_new_content_count >= 8:
                    print(f"[Worker {self.worker_id}] Hoàn tất thu thập URLs sau 8 lần không có nội dung mới")
                    break
            else:
                no_new_content_count = 0
                previous_content_length = current_content_length

            # Cập nhật chiều cao trang mới
            new_height = self.driver.execute_script("return document.documentElement.scrollHeight")
            if new_height != last_height:
                last_height = new_height
                no_new_content_count = 0
            elif abs(actual_height - last_height) < 100:
                time.sleep(SCROLL_PAUSE_TIME * 2)
                if abs(actual_height - self.driver.execute_script("return document.documentElement.scrollHeight")) < 100:
                    print(f"[Worker {self.worker_id}] Đã đến cuối trang")
                    break

        print(f"[Worker {self.worker_id}] Tổng cộng đã thu thập được {len(group_links)} URLs độc nhất")
        return list(group_links)

def save_group_urls(urls, keyword, output_dir, worker_id):
    file_path = os.path.join(output_dir, f'collected_urls_worker_{worker_id}.json')
    data = {
        'keyword': keyword,
        'timestamp': datetime.now().isoformat(),
        'urls': urls
    }

    try:
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
                if not isinstance(existing_data, list):
                    existing_data = [existing_data]
        else:
            existing_data = []
            
        existing_data.append(data)
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(existing_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"Error saving URLs to JSON: {str(e)}")

def save_failed_urls(failed_urls, output_dir, worker_id):
    file_path = os.path.join(output_dir, f'failed_urls_worker_{worker_id}.json')
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(failed_urls, f, ensure_ascii=False, indent=2)
        print(f"[Worker {worker_id}] Đã lưu {len(failed_urls)} URLs thất bại vào file")
    except Exception as e:
        print(f"Error saving failed URLs: {str(e)}")

def convert_members_count(count_text):
    try:
        # Remove "thành viên", "members" text
        count_text = count_text.lower().replace('thành viên', '').replace('members', '').strip()
        
        # Handle "triệu" (Vietnamese millions)
        if 'triệu' in count_text.lower():
            number = float(count_text.lower().replace('triệu', '').replace(',', '.').strip())
            return int(number * 1000000)
        
        # Handle K (thousands)
        if 'k' in count_text.lower():
            number = float(count_text.lower().replace('k', '').replace(',', '.'))
            return int(number * 1000)
        
        # Handle M (millions) 
        if 'm' in count_text.lower():
            number = float(count_text.lower().replace('m', '').replace(',', '.'))
            return int(number * 1000000)
            
        # Handle B (billions)
        if 'b' in count_text.lower():
            number = float(count_text.lower().replace('b', '').replace(',', '.'))
            return int(number * 1000000000)
            
        # Handle regular numbers
        return int(count_text.replace(',', ''))
    except:
        return 0

def process_single_group(chrome_instance, group_url, search_keywords, output_file, lock, retry_count=0):
    print(f"[Worker {chrome_instance.worker_id}] Đang xử lý nhóm: {group_url}")
    try:
        chrome_instance.driver.get(group_url)
        time.sleep(random.uniform(2, 3))
		
		# Thêm xử lý popup block
        if chrome_instance.close_block_popup():
            print(f"[Worker {chrome_instance.worker_id}] Đã đóng popup block")
            time.sleep(random.uniform(1, 2))


        # Check if page loaded properly
        if "content not found" in chrome_instance.driver.page_source.lower() or "không tìm thấy nội dung" in chrome_instance.driver.page_source.lower():
            if retry_count < MAX_RETRIES:
                time.sleep(random.uniform(1, 3))
                return process_single_group(chrome_instance, group_url, search_keywords, output_file, lock, retry_count + 1)
            else:
                raise Exception("Không thể tải trang sau nhiều lần thử")

        # Get group name with explicit wait and retry
        try:
            group_name_element = WebDriverWait(chrome_instance.driver, 5).until(
                EC.presence_of_element_located((By.XPATH, "//h1//a[contains(@class, 'x16tdsg8')]"))
            )
            group_name = group_name_element.text.strip()
        except:
            if retry_count < MAX_RETRIES:
                time.sleep(random.uniform(1, 2))
                return process_single_group(chrome_instance, group_url, search_keywords, output_file, lock, retry_count + 1)
            else:
                raise Exception("Không thể lấy tên nhóm")

        # Get group type with retry
        try:
            group_type_element = WebDriverWait(chrome_instance.driver, 5).until(
                EC.presence_of_element_located((By.XPATH, "//div[contains(text(), 'Nhóm') and contains(@class, 'x1n2onr6')]"))
            )
            group_type = group_type_element.text.strip()
        except:
            group_type = "Unknown"

        # Get members count with retry
        try:
            members_element = WebDriverWait(chrome_instance.driver, 5).until(
                EC.presence_of_element_located((By.XPATH, "//a[contains(@href, '/members/')]"))
            )
            members_text = members_element.text
            members_match = re.search(r'([\d,\.]+[KMB]?) thành viên', members_text)
            members_count = members_match.group(1) if members_match else "0"
        except:
            if retry_count < MAX_RETRIES:
                time.sleep(random.uniform(1, 2))
                return process_single_group(chrome_instance, group_url, search_keywords, output_file, lock, retry_count + 1)
            else:
                raise Exception("Không thể lấy số lượng thành viên")

        # Convert members count to actual number
        numeric_members_count = convert_members_count(members_count)

        # Random scroll 3-6 times
        num_scrolls = random.randint(3, 6)
        for _ in range(num_scrolls):
            chrome_instance.driver.execute_script("window.scrollBy(0, 600)")
            time.sleep(random.uniform(0.5, 1))

        group_data = {
            'keyword_search': search_keywords,
            'group_name': group_name,
            'group_link': group_url,
            'group_type': group_type,
            'members_count': numeric_members_count
        }

        # Save with lock and verification
        with lock:
            try:
                write_header = not os.path.exists(output_file)
                with open(output_file, 'a', newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnames=list(group_data.keys()))
                    if write_header:
                        writer.writeheader()
                    writer.writerow(group_data)
                print(f"[Worker {chrome_instance.worker_id}] Đã lưu thành công nhóm: {group_name}")
                return True
            except Exception as e:
                print(f"[Worker {chrome_instance.worker_id}] Lỗi lưu dữ liệu: {str(e)}")
                return False

    except Exception as e:
        print(f"[Worker {chrome_instance.worker_id}] Lỗi xử lý nhóm {group_url}: {str(e)}")
        chrome_instance.failed_urls.append({
            'url': group_url,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        })
        if retry_count < MAX_RETRIES:
            time.sleep(random.uniform(1, 2))
            return process_single_group(chrome_instance, group_url, search_keywords, output_file, lock, retry_count + 1)
        return False

def worker_process(chrome_instance: ChromeInstance, keywords: List[str], processed_items: set, processed_groups: set, lock, output_dir):
    try:
        driver = chrome_instance.create_driver()
        output_file = os.path.join(output_dir, 'groups_data.csv')
        retry_count = 0

        while True:
            if chrome_instance.check_checkpoint():
                chrome_instance.is_checkpoint = True
                print(f"[Worker {chrome_instance.worker_id}] Tài khoản bị checkpoint, dừng worker")
                break

            if chrome_instance.check_blocked():
                chrome_instance.is_blocked = True
                print(f"[Worker {chrome_instance.worker_id}] Tài khoản bị chặn tạm thời, dừng worker")
                break

            with lock:
                if len(keywords) < 2:
                    break
                selected_keywords = random.sample(keywords, 2)
                search_keywords = ' '.join(selected_keywords)

            if search_keywords in processed_items:
                continue

            try:
                encoded_keywords = '%20'.join(search_keywords.split())
                search_url = f"https://www.facebook.com/groups/search/groups?q={encoded_keywords}&filters=eyJwdWJsaWNfZ3JvdXBzOjAiOiJ7XCJuYW1lXCI6XCJwdWJsaWNfZ3JvdXBzXCIsXCJhcmdzXCI6XCJcIn0ifQ%3D%3D"

                print(f"\n[Worker {chrome_instance.worker_id}] Tìm kiếm với từ khóa: {search_keywords}")
                driver.get(search_url)
                time.sleep(random.uniform(2, 3))

                # Thu thập URLs với tracking
                group_urls = chrome_instance.scroll_and_collect_groups()
                if not group_urls:
                    print(f"[Worker {chrome_instance.worker_id}] Không tìm thấy URLs cho từ khóa: {search_keywords}")
                    continue

                # Lưu URLs đã thu thập
                save_group_urls(group_urls, search_keywords, output_dir, chrome_instance.worker_id)

                # Tracking progress
                total_urls = len(group_urls)
                processed_urls = 0
                failed_urls = 0

                print(f"[Worker {chrome_instance.worker_id}] Bắt đầu xử lý {total_urls} nhóm...")
                
                for idx, group_url in enumerate(group_urls, 1):
                    if group_url in processed_groups:
                        processed_urls += 1
                        continue

                    print(f"[Worker {chrome_instance.worker_id}] Đang xử lý nhóm {idx}/{total_urls}")
                    
                    if process_single_group(chrome_instance, group_url, search_keywords, output_file, lock):
                        with lock:
                            processed_groups.add(group_url)
                            chrome_instance.processed_groups.add(group_url)
                        processed_urls += 1
                    else:
                        failed_urls += 1

                    time.sleep(random.uniform(MIN_DELAY_BETWEEN_REQUESTS, MAX_DELAY_BETWEEN_REQUESTS))

                print(f"[Worker {chrome_instance.worker_id}] Kết quả xử lý keyword '{search_keywords}':")
                print(f"- Tổng số URLs: {total_urls}")
                print(f"- Đã xử lý: {processed_urls}")
                print(f"- Thất bại: {failed_urls}")

                chrome_instance.processed_count += 1
                with lock:
                    processed_items.add(search_keywords)
                    if chrome_instance.processed_count % 10 == 0:
                        save_progress(processed_items, processed_groups, output_dir)

            except Exception as e:
                print(f"[Worker {chrome_instance.worker_id}] Lỗi xử lý từ khóa {search_keywords}: {str(e)}")
                retry_count += 1
                if retry_count >= MAX_RETRIES:
                    print(f"[Worker {chrome_instance.worker_id}] Đã vượt quá số lần thử lại cho phép")
                    break
                time.sleep(random.uniform(2, 4))

    except Exception as e:
        print(f"[Worker {chrome_instance.worker_id}] Lỗi: {str(e)}")
    finally:
        if chrome_instance.failed_urls:
            save_failed_urls(chrome_instance.failed_urls, output_dir, chrome_instance.worker_id)
        if chrome_instance.driver:
            chrome_instance.driver.quit()

def load_keywords(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f.readlines() if line.strip()]

def load_progress(output_dir):
    try:
        with open(os.path.join(output_dir, 'scan_progress.json'), 'r') as f:
            data = json.load(f)
            return data.get('processed_items', []), set(data.get('processed_groups', []))
    except:
        return [], set()

def save_progress(processed_items, processed_groups, output_dir):
    with open(os.path.join(output_dir, 'scan_progress.json'), 'w') as f:
        json.dump({
            'processed_items': list(processed_items),
            'processed_groups': list(processed_groups)
        }, f)

def main():
    input_file = r'C:\Users\Admin\Downloads\Tìm Group Công Khai\Keyword.txt'
    output_dir = r'C:\Users\Admin\Downloads\Tìm Group Công Khai\output'

    os.makedirs(output_dir, exist_ok=True)

    keywords = load_keywords(input_file)
    processed_items, processed_groups = load_progress(output_dir)
    processed_items = set(processed_items)

    print(f"Tổng số keywords: {len(keywords)}")

    chrome_instances = [
        ChromeInstance(debug_port=9222, worker_id=1),
        ChromeInstance(debug_port=9223, worker_id=2),
        ChromeInstance(debug_port=9224, worker_id=3),
        ChromeInstance(debug_port=9225, worker_id=4),
        ChromeInstance(debug_port=9226, worker_id=5),
        ChromeInstance(debug_port=9227, worker_id=6)
    ]

    lock = threading.Lock()

    threads = []
    for chrome_instance in chrome_instances:
        thread = threading.Thread(
            target=worker_process,
            args=(chrome_instance, keywords, processed_items, processed_groups, lock, output_dir)
        )
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()

    print("\nHoàn tất xử lý!")

if __name__ == "__main__":
    main()

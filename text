import time
from datetime import datetime
import random
import json
import csv
import re
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchWindowException
from collections import defaultdict, deque
from typing import Dict, List
from selenium.webdriver.common.action_chains import ActionChains
import os
import threading
import math


# Configuration parameters
MIN_DELAY_BETWEEN_REQUESTS = 0.5
MAX_DELAY_BETWEEN_REQUESTS = 1
MAX_RETRIES = 3
PAGE_LOAD_TIMEOUT = 10
TAB_SWITCH_DELAY = 0.2
MAX_TABS = 5
SCROLL_PAUSE_TIME = 2

def calculate_std_dev(values):
    n = len(values)
    if n < 2:
        return 0
    mean = sum(values) / n
    squared_diff_sum = sum((x - mean) ** 2 for x in values)
    return math.sqrt(squared_diff_sum / (n - 1))

class ChromeInstance:
    def __init__(self, debug_port, worker_id):
        self.debug_port = debug_port
        self.worker_id = worker_id
        self.driver = None
        self.processed_count = 0
        self.is_checkpoint = False
        self.is_blocked = False
        self.processed_groups = set()
        self.collected_urls = []
        self.failed_urls = []

    def save_progress(self, keyword, urls, current_index, output_dir):
        progress_file = os.path.join(output_dir, f'progress_worker_{self.worker_id}.json')
        progress_data = {
            'keyword': keyword,
            'urls': urls,
            'current_index': current_index,
            'timestamp': datetime.now().isoformat()
        }
        try:
            with open(progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"[Worker {self.worker_id}] Lỗi lưu tiến độ: {str(e)}")

    def load_progress(self, output_dir):
        progress_file = os.path.join(output_dir, f'progress_worker_{self.worker_id}.json')
        try:
            if os.path.exists(progress_file):
                with open(progress_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"[Worker {self.worker_id}] Lỗi đọc tiến độ: {str(e)}")
        return None

    def clear_progress(self, output_dir):
        progress_file = os.path.join(output_dir, f'progress_worker_{self.worker_id}.json')
        try:
            if os.path.exists(progress_file):
                os.remove(progress_file)
        except Exception as e:
            print(f"[Worker {self.worker_id}] Lỗi xóa file tiến độ: {str(e)}")

    def setup_chrome_driver(self):
        chrome_options = Options()
        chrome_options.add_experimental_option("debuggerAddress", f"127.0.0.1:{self.debug_port}")
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-notifications')
        chrome_options.add_argument('--disable-infobars')
        chrome_options.add_argument('--lang=vi')
        return chrome_options

    def create_driver(self):
        try:
            options = self.setup_chrome_driver()
            service = Service()
            self.driver = webdriver.Chrome(service=service, options=options)
            self.driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)
            return self.driver
        except Exception as e:
            raise Exception(f"Lỗi khởi tạo Chrome WebDriver port {self.debug_port}: {str(e)}")

    def close_block_popup(self):
        try:
            popup = self.driver.find_element(By.XPATH, "//div[contains(@class, 'x1ey2m1c') and contains(@class, 'xds687c') and contains(@class, 'x17qophe') and contains(@class, 'xg01cxk') and contains(@class, 'x47corl') and contains(@class, 'x10l6tqk') and contains(@class, 'x13vifvy') and contains(@class, 'x1ebt8du') and contains(@class, 'x19991ni') and contains(@class, 'x1dhq9h') and contains(@class, 'xzolkzo') and contains(@class, 'x12go9s9') and contains(@class, 'x1rnf11y') and contains(@class, 'xprq8jg')]")
            actions = ActionChains(self.driver)
            actions.double_click(popup).perform()
            time.sleep(1)
            return True
        except:
            return False

    def check_checkpoint(self):
        try:
            current_url = self.driver.current_url.lower()
            checkpoint_indicators = [
                "facebook.com/checkpoint",
                "facebook.com/recover",
                "facebook.com/login",
                "/login/?next",
                "checkpoint/?next"
            ]
            return any(indicator in current_url for indicator in checkpoint_indicators)
        except:
            return False

    def check_blocked(self):
        try:
            blocked_text = "Bạn tạm thời bị chặn"
            blocked_text_2 = "Có vẻ như bạn đang dùng nhầm tính năng này do sử dụng quá nhanh"
            page_source = self.driver.page_source
            return blocked_text in page_source or blocked_text_2 in page_source
        except:
            return False

    def check_suspicious(self):
        try:
            # Theo dõi URL hiện tại
            current_url = self.driver.current_url.lower()
            
            # Nếu URL chứa checkpoint suspicious
            if "facebook.com/checkpoint/601051028565049" in current_url:
                # Tìm nút "Bỏ qua" hoặc "Dismiss"
                skip_button = self.driver.find_elements(By.XPATH, "//span[contains(@class, 'x1lliihq') and contains(@class, 'x6ikm8r') and contains(@class, 'x10wlt62') and contains(@class, 'x1n2onr6') and contains(@class, 'xlyipyv') and contains(@class, 'xuxw1ft') and (contains(text(), 'Bỏ qua') or contains(text(), 'Dismiss'))]")
                
                if skip_button:
                    try:
                        actions = ActionChains(self.driver)
                        actions.double_click(skip_button[0]).perform()
                        print(f"[Worker {self.worker_id}] Đã click nút Bỏ qua trên trang Checkpoint")
                        time.sleep(1)
                        return True
                    except Exception as e:
                        print(f"[Worker {self.worker_id}] Lỗi khi click nút Bỏ qua: {str(e)}")
                        return False
            return False
        except Exception as e:
            print(f"[Worker {self.worker_id}] Lỗi khi kiểm tra trang suspicious: {str(e)}")
            return False

    # Thêm hàm mới để theo dõi URL
    def url_monitor(self):
        """Hàm theo dõi URL và xử lý checkpoint"""
        try:
            current_url = self.driver.current_url
            if "facebook.com/checkpoint/601051028565049" in current_url.lower():
                self.check_suspicious()
        except Exception as e:
            print(f"[Worker {self.worker_id}] Lỗi khi theo dõi URL: {str(e)}")

    def scroll_and_collect_groups(self):
        print(f"[Worker {self.worker_id}] Bắt đầu scroll và thu thập URLs...")
        group_links = []  # Đổi từ set sang list để duy trì thứ tự
        group_links_set = set()  # Dùng set để kiểm tra trùng lặp
        no_new_content_count = 0
        previous_unique_count = 0
        last_height = self.driver.execute_script("return document.documentElement.scrollHeight")
        
        while True:
            current_height = self.driver.execute_script("return window.pageYOffset")
            target_height = min(current_height + 300, last_height)
            self.driver.execute_script(f"window.scrollTo(0, {target_height});")
            time.sleep(0.3)

            time.sleep(SCROLL_PAUSE_TIME)
            
            actual_height = self.driver.execute_script("return window.pageYOffset")
            if abs(actual_height - target_height) > 50:
                self.driver.execute_script(f"window.scrollTo(0, {target_height});")
                time.sleep(0.3)

            elements = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='/groups/']")
            
            new_urls_added = 0
            for element in elements:
                try:
                    href = element.get_attribute('href')
                    if href and re.search(r'facebook\.com/groups/\d+/?$', href):
                        clean_url = re.sub(r'\?.*$', '', href)
                        if clean_url not in self.processed_groups and clean_url not in group_links_set:
                            group_links.append(clean_url)  # Thêm vào list để giữ thứ tự
                            group_links_set.add(clean_url)  # Thêm vào set để kiểm tra trùng lặp
                            new_urls_added += 1
                except:
                    continue
            
            current_unique_count = len(group_links)
            print(f"[Worker {self.worker_id}] Đã tìm thấy {new_urls_added} nhóm mới (Tổng: {current_unique_count} nhóm độc nhất)")
            
            if current_unique_count == previous_unique_count:
                no_new_content_count += 1
                print(f"[Worker {self.worker_id}] Không tìm thấy nhóm mới... ({no_new_content_count}/8)")
                if no_new_content_count >= 8:
                    print(f"[Worker {self.worker_id}] Hoàn tất thu thập URLs sau 8 lần không có nhóm mới")
                    break
            else:
                no_new_content_count = 0
                previous_unique_count = current_unique_count

            new_height = self.driver.execute_script("return document.documentElement.scrollHeight")
            if new_height != last_height:
                last_height = new_height
                no_new_content_count = 0
            elif abs(actual_height - last_height) < 100:
                time.sleep(SCROLL_PAUSE_TIME * 2)
                if abs(actual_height - self.driver.execute_script("return document.documentElement.scrollHeight")) < 100:
                    print(f"[Worker {self.worker_id}] Đã đến cuối trang")
                    break

        print(f"[Worker {self.worker_id}] Tổng cộng đã thu thập được {len(group_links)} URLs độc nhất")
        return group_links  # Trả về list thay vì set

def save_group_urls(urls, keyword, output_dir, worker_id):
    file_path = os.path.join(output_dir, f'collected_urls_worker_{worker_id}.json')
    data = {
        'keyword': keyword,
        'timestamp': datetime.now().isoformat(),
        'urls': urls,
        'total_urls': len(urls)
    }

    try:
        existing_data = []
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                try:
                    existing_data = json.load(f)
                    if not isinstance(existing_data, list):
                        existing_data = [existing_data]
                except json.JSONDecodeError:
                    existing_data = []
        
        # Tạo set của URLs đã tồn tại
        existing_urls = set()
        for entry in existing_data:
            existing_urls.update(entry.get('urls', []))
        
        # Lọc URLs mới và giữ nguyên thứ tự
        new_urls = [url for url in urls if url not in existing_urls]
        if new_urls:
            data['urls'] = new_urls
            data['total_urls'] = len(new_urls)
            existing_data.append(data)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(existing_data, f, ensure_ascii=False, indent=2)
            print(f"[Worker {worker_id}] Đã lưu {len(new_urls)} URLs mới vào file")
        else:
            print(f"[Worker {worker_id}] Không có URLs mới để lưu")
            
    except Exception as e:
        print(f"Error saving URLs to JSON: {str(e)}")

def save_failed_urls(failed_urls, output_dir, worker_id):
    file_path = os.path.join(output_dir, f'failed_urls_worker_{worker_id}.json')
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump({
                'failed_urls': failed_urls,
                'total_failed': len(failed_urls),
                'timestamp': datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)
        print(f"[Worker {worker_id}] Đã lưu {len(failed_urls)} URLs thất bại vào file")
    except Exception as e:
        print(f"Error saving failed URLs: {str(e)}")

def convert_members_count(count_text):
    try:
        # Remove "thành viên", "members" text
        count_text = count_text.lower().replace('thành viên', '').replace('members', '').strip()
        
        # Handle "triệu" (Vietnamese millions)
        if 'triệu' in count_text.lower():
            number = float(count_text.lower().replace('triệu', '').replace(',', '.').strip())
            return int(number * 1000000)
        
        # Handle K (thousands)
        if 'k' in count_text.lower():
            number = float(count_text.lower().replace('k', '').replace(',', '.'))
            return int(number * 1000)
        
        # Handle M (millions) 
        if 'm' in count_text.lower():
            number = float(count_text.lower().replace('m', '').replace(',', '.'))
            return int(number * 1000000)
            
        # Handle B (billions)
        if 'b' in count_text.lower():
            number = float(count_text.lower().replace('b', '').replace(',', '.'))
            return int(number * 1000000000)
            
        # Handle regular numbers
        return int(count_text.replace(',', ''))
    except:
        return 0

def process_single_group(chrome_instance, group_url, search_keywords, output_file, lock, retry_count=0):
    print(f"[Worker {chrome_instance.worker_id}] Đang xử lý nhóm: {group_url}")
    try:
        chrome_instance.driver.get(group_url)
        chrome_instance.url_monitor() 
        
        time.sleep(random.uniform(2, 3))

        def handle_popup():
            """Xử lý popup block"""
            if chrome_instance.close_block_popup():
                print(f"[Worker {chrome_instance.worker_id}] Đã đóng popup block")
                time.sleep(random.uniform(1, 2))
                return True
            return False

        # Check popup sau khi load trang
        time.sleep(1)
        handle_popup()

        if "content not found" in chrome_instance.driver.page_source.lower() or "không tìm thấy nội dung" in chrome_instance.driver.page_source.lower():
            if retry_count < MAX_RETRIES:
                time.sleep(random.uniform(1, 3))
                return process_single_group(chrome_instance, group_url, search_keywords, output_file, lock, retry_count + 1)
            else:
                raise Exception("Không thể tải trang sau nhiều lần thử")

        try:
            group_name_element = WebDriverWait(chrome_instance.driver, 2).until(
                EC.presence_of_element_located((By.XPATH, "//h1//a[contains(@class, 'x16tdsg8')]"))
            )
            group_name = group_name_element.text.strip()
        except:
            if retry_count < MAX_RETRIES:
                time.sleep(random.uniform(1, 2))
                return process_single_group(chrome_instance, group_url, search_keywords, output_file, lock, retry_count + 1)
            else:
                raise Exception("Không thể lấy tên nhóm")

        try:
            group_type_element = WebDriverWait(chrome_instance.driver, 1).until(
                EC.presence_of_element_located((By.XPATH, "//div[contains(text(), 'Nhóm') and contains(@class, 'x1n2onr6')]"))
            )
            group_type = group_type_element.text.strip()
        except:
            group_type = "Unknown"

        try:
            members_element = WebDriverWait(chrome_instance.driver, 1).until(
                EC.presence_of_element_located((By.XPATH, "//a[contains(@href, '/members/')]"))
            )
            members_text = members_element.text
            members_match = re.search(r'([\d,\.]+[KMB]?) thành viên', members_text)
            members_count = members_match.group(1) if members_match else "0"
        except:
            if retry_count < MAX_RETRIES:
                time.sleep(random.uniform(1, 2))
                return process_single_group(chrome_instance, group_url, search_keywords, output_file, lock, retry_count + 1)
            else:
                raise Exception("Không thể lấy số lượng thành viên")

        numeric_members_count = convert_members_count(members_count)

        try:
            can_post = "No"
            try:
                post_box_vn = WebDriverWait(chrome_instance.driver, 2).until(
                    EC.presence_of_element_located((By.XPATH, '//span[@class="x1lliihq x6ikm8r x10wlt62 x1n2onr6" and contains(text(), "Bạn viết gì đi...")]'))
                )
                can_post = "Yes"
            except:
                try:
                    post_box_en = WebDriverWait(chrome_instance.driver, 2).until(
                        EC.presence_of_element_located((By.XPATH, '//span[@class="x1lliihq x6ikm8r x10wlt62 x1n2onr6" and contains(text(), "Write something...")]'))
                    )
                    can_post = "Yes"
                except:
                    can_post = "No"
        except Exception as e:
            print(f"[Worker {chrome_instance.worker_id}] Lỗi khi kiểm tra quyền đăng bài: {str(e)}")
            can_post = "No"

        # Thực hiện scroll
        num_scrolls = random.randint(1, 3)
        for i in range(num_scrolls):
            chrome_instance.driver.execute_script("window.scrollBy(0, 600)")
            time.sleep(random.uniform(0.5, 1))
            
            # Check popup sau lần scroll đầu tiên
            if i == 0:
                time.sleep(1)
                handle_popup()

        group_data = {
            'keyword_search': search_keywords,
            'group_name': group_name,
            'group_link': group_url,
            'group_type': group_type,
            'members_count': numeric_members_count,
            'can_post': can_post
        }

        with lock:
            try:
                write_header = not os.path.exists(output_file)
                with open(output_file, 'a', newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnames=list(group_data.keys()))
                    if write_header:
                        writer.writeheader()
                    writer.writerow(group_data)
                print(f"[Worker {chrome_instance.worker_id}] Đã lưu thành công nhóm: {group_name}")
                return True
            except Exception as e:
                print(f"[Worker {chrome_instance.worker_id}] Lỗi lưu dữ liệu: {str(e)}")
                return False

    except Exception as e:
        print(f"[Worker {chrome_instance.worker_id}] Lỗi xử lý nhóm {group_url}: {str(e)}")
        chrome_instance.failed_urls.append({
            'url': group_url,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        })
        if retry_count < MAX_RETRIES:
            time.sleep(random.uniform(1, 2))
            return process_single_group(chrome_instance, group_url, search_keywords, output_file, lock, retry_count + 1)
        return False

def worker_process(chrome_instance: ChromeInstance, keywords: List[str], processed_items: set, processed_groups: set, lock, output_dir):
    try:
        driver = chrome_instance.create_driver()
        output_file = os.path.join(output_dir, 'groups_data.csv')
        retry_count = 0

        # Kiểm tra và load tiến độ trước đó
        progress_data = chrome_instance.load_progress(output_dir)
        if progress_data:
            print(f"[Worker {chrome_instance.worker_id}] Tìm thấy tiến độ trước đó:")
            print(f"- Keyword: {progress_data['keyword']}")
            print(f"- Số URL còn lại: {len(progress_data['urls']) - progress_data['current_index']}")
            
            # Tiếp tục xử lý các URL còn lại từ lần trước
            total_remaining = len(progress_data['urls']) - progress_data['current_index']
            processed_urls = 0
            failed_urls = 0
            
            for idx, group_url in enumerate(progress_data['urls'][progress_data['current_index']:], 1):
                if group_url in processed_groups:
                    processed_urls += 1
                    continue

                print(f"[Worker {chrome_instance.worker_id}] Đang xử lý nhóm còn lại {idx}/{total_remaining}")
                
                if process_single_group(chrome_instance, group_url, progress_data['keyword'], output_file, lock):
                    with lock:
                        processed_groups.add(group_url)
                        chrome_instance.processed_groups.add(group_url)
                    processed_urls += 1
                else:
                    failed_urls += 1

                # Lưu tiến độ sau mỗi URL
                chrome_instance.save_progress(
                    progress_data['keyword'],
                    progress_data['urls'],
                    progress_data['current_index'] + idx,
                    output_dir
                )

                time.sleep(random.uniform(MIN_DELAY_BETWEEN_REQUESTS, MAX_DELAY_BETWEEN_REQUESTS))

            # Xóa file tiến độ sau khi hoàn thành
            chrome_instance.clear_progress(output_dir)
            print(f"[Worker {chrome_instance.worker_id}] Đã hoàn thành xử lý các URL còn lại")

        while True:
            if chrome_instance.check_checkpoint():
                chrome_instance.is_checkpoint = True
                print(f"[Worker {chrome_instance.worker_id}] Tài khoản bị checkpoint, dừng worker")
                break

            if chrome_instance.check_blocked():
                chrome_instance.is_blocked = True
                print(f"[Worker {chrome_instance.worker_id}] Tài khoản bị chặn tạm thời, dừng worker")
                break
            
            if chrome_instance.check_suspicious():
                print(f"[Worker {chrome_instance.worker_id}] Đã xử lý cảnh báo suspicious")
                time.sleep(random.uniform(1, 2))
                continue

            with lock:
                if len(keywords) < 2:
                    break
                selected_keywords = random.sample(keywords, 2)
                search_keywords = ' '.join(selected_keywords)

            if search_keywords in processed_items:
                continue

            try:
                encoded_keywords = '%20'.join(search_keywords.split())
                search_url = f"https://www.facebook.com/groups/search/groups?q={encoded_keywords}&filters=eyJwdWJsaWNfZ3JvdXBzOjAiOiJ7XCJuYW1lXCI6XCJwdWJsaWNfZ3JvdXBzXCIsXCJhcmdzXCI6XCJcIn0ifQ%3D%3D"

                print(f"\n[Worker {chrome_instance.worker_id}] Tìm kiếm với từ khóa: {search_keywords}")
                driver.get(search_url)
                time.sleep(random.uniform(2, 3))

                group_urls = chrome_instance.scroll_and_collect_groups()
                if not group_urls:
                    print(f"[Worker {chrome_instance.worker_id}] Không tìm thấy URLs cho từ khóa: {search_keywords}")
                    continue

                save_group_urls(group_urls, search_keywords, output_dir, chrome_instance.worker_id)

                total_urls = len(group_urls)
                processed_urls = 0
                failed_urls = 0

                print(f"[Worker {chrome_instance.worker_id}] Bắt đầu xử lý {total_urls} nhóm...")
                
                for idx, group_url in enumerate(group_urls, 1):
                    if group_url in processed_groups:
                        processed_urls += 1
                        continue

                    print(f"[Worker {chrome_instance.worker_id}] Đang xử lý nhóm {idx}/{total_urls}")
                    
                    # Lưu tiến độ trước khi xử lý mỗi URL
                    chrome_instance.save_progress(
                        search_keywords,
                        group_urls,
                        idx - 1,
                        output_dir
                    )
                    
                    if process_single_group(chrome_instance, group_url, search_keywords, output_file, lock):
                        with lock:
                            processed_groups.add(group_url)
                            chrome_instance.processed_groups.add(group_url)
                        processed_urls += 1
                    else:
                        failed_urls += 1

                    time.sleep(random.uniform(MIN_DELAY_BETWEEN_REQUESTS, MAX_DELAY_BETWEEN_REQUESTS))

                # Xóa file tiến độ sau khi hoàn thành keyword
                chrome_instance.clear_progress(output_dir)

                print(f"[Worker {chrome_instance.worker_id}] Kết quả xử lý keyword '{search_keywords}':")
                print(f"- Tổng số URLs: {total_urls}")
                print(f"- Đã xử lý: {processed_urls}")
                print(f"- Thất bại: {failed_urls}")

                chrome_instance.processed_count += 1
                with lock:
                    processed_items.add(search_keywords)
                    if chrome_instance.processed_count % 10 == 0:
                        save_progress(processed_items, processed_groups, output_dir)

            except Exception as e:
                print(f"[Worker {chrome_instance.worker_id}] Lỗi xử lý từ khóa {search_keywords}: {str(e)}")
                retry_count += 1
                if retry_count >= MAX_RETRIES:
                    print(f"[Worker {chrome_instance.worker_id}] Đã vượt quá số lần thử lại cho phép")
                    break
                time.sleep(random.uniform(2, 4))

    except Exception as e:
        print(f"[Worker {chrome_instance.worker_id}] Lỗi: {str(e)}")
    finally:
        if chrome_instance.failed_urls:
            save_failed_urls(chrome_instance.failed_urls, output_dir, chrome_instance.worker_id)
        if chrome_instance.driver:
            chrome_instance.driver.quit()

def load_keywords(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f.readlines() if line.strip()]

def load_progress(output_dir):
    try:
        with open(os.path.join(output_dir, 'scan_progress.json'), 'r') as f:
            data = json.load(f)
            return data.get('processed_items', []), set(data.get('processed_groups', []))
    except:
        return [], set()

def save_progress(processed_items, processed_groups, output_dir):
    with open(os.path.join(output_dir, 'scan_progress.json'), 'w') as f:
        json.dump({
            'processed_items': list(processed_items),
            'processed_groups': list(processed_groups)
        }, f)

def main():
    input_file = r'C:\Users\Admin\Downloads\Tìm Group Công Khai\Keyword.txt'
    output_dir = r'C:\Users\Admin\Downloads\Tìm Group Công Khai\output'

    os.makedirs(output_dir, exist_ok=True)

    keywords = load_keywords(input_file)
    processed_items, processed_groups = load_progress(output_dir)
    processed_items = set(processed_items)

    print(f"Tổng số keywords: {len(keywords)}")

    chrome_instances = [
        ChromeInstance(debug_port=9222, worker_id=1),
        ChromeInstance(debug_port=9223, worker_id=2),
        ChromeInstance(debug_port=9224, worker_id=3),
        ChromeInstance(debug_port=9225, worker_id=4),
        ChromeInstance(debug_port=9226, worker_id=5),
        ChromeInstance(debug_port=9227, worker_id=6)
    ]

    lock = threading.Lock()

    threads = []
    for chrome_instance in chrome_instances:
        thread = threading.Thread(
            target=worker_process,
            args=(chrome_instance, keywords, processed_items, processed_groups, lock, output_dir)
        )
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()

    print("\nHoàn tất xử lý!")

if __name__ == "__main__":
    main()
